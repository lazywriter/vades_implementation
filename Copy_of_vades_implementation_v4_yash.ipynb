{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazywriter/vades_implementation/blob/main/Copy_of_vades_implementation_v4_yash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iznbkVFBG8yl",
        "outputId": "381ba179-b766-4214-b31b-6b9641662097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m149.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2588917736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mDRIVE_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/final_year_project_books_folder\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SETUP\n",
        "# ============================================================================\n",
        "\n",
        "!pip install -q sentence-transformers spacy textstat joblib scikit-learn beautifulsoup4\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, re, string, random, joblib\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from scipy.spatial.distance import pdist\n",
        "import spacy, textstat\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/final_year_project_books_folder\"\n",
        "BOOKS_DIR = os.path.join(DRIVE_ROOT, \"gutenberg_books\")\n",
        "CACHE_DIR = os.path.join(DRIVE_ROOT, \"feat_cache\")\n",
        "CKPT_DIR = os.path.join(DRIVE_ROOT, \"vades_final_rebuild\")\n",
        "SPLIT_DIR = os.path.join(DRIVE_ROOT, \"splits_final\")\n",
        "OUTPUT_CSV = os.path.join(DRIVE_ROOT, \"gutenberg_dataset_clean.csv\")\n",
        "\n",
        "for d in [CACHE_DIR, CKPT_DIR, SPLIT_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "RND = 42\n",
        "random.seed(RND)\n",
        "np.random.seed(RND)\n",
        "torch.manual_seed(RND)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING (WITH CACHING)\n",
        "# ============================================================================\n",
        "\n",
        "def extract_text_from_html(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        soup = BeautifulSoup(f, \"html.parser\")\n",
        "        for tag in soup([\"table\", \"script\", \"style\"]): tag.extract()\n",
        "        return soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "def clean_gutenberg_text(text):\n",
        "    text = re.sub(r'(?is).*?(the project gutenberg ebook of )', '', text, count=1)\n",
        "    text = re.sub(r'(?is)end of (the )?project gutenberg ebook.*', '', text)\n",
        "    text = re.sub(r'(?is)\\*\\*\\*.*?project gutenberg.*?\\*\\*\\*', '', text)\n",
        "    text = re.sub(r'(?i)(produced by|transcribed from).*?\\n', '', text)\n",
        "    text = re.sub(r'\\b(CHAPTER|Chapter)\\s+[A-Z0-9IVX]+\\b', '', text)\n",
        "    text = re.sub(r'[_*#~-]{2,}', ' ', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def chunk_text(text, size=4000, overlap=500):\n",
        "    chunks, start = [], 0\n",
        "    while start < len(text):\n",
        "        chunks.append(text[start:start+size])\n",
        "        start += size - overlap\n",
        "    return chunks\n",
        "\n",
        "if os.path.exists(OUTPUT_CSV):\n",
        "    df = pd.read_csv(OUTPUT_CSV)\n",
        "    print(f\"✓ Loaded {len(df)} chunks\")\n",
        "else:\n",
        "    data = []\n",
        "    for author in tqdm(os.listdir(BOOKS_DIR)):\n",
        "        folder = os.path.join(BOOKS_DIR, author)\n",
        "        if not os.path.isdir(folder): continue\n",
        "        for fname in os.listdir(folder):\n",
        "            if not fname.endswith(\".html\"): continue\n",
        "            try:\n",
        "                raw = extract_text_from_html(os.path.join(folder, fname))\n",
        "                clean = clean_gutenberg_text(raw)\n",
        "                for chunk in chunk_text(clean):\n",
        "                    if chunk.strip():\n",
        "                        data.append({\"author\": author, \"file_name\": fname, \"text\": chunk})\n",
        "            except: pass\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(OUTPUT_CSV, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BOOK-LEVEL SPLIT (NO DATA LEAKAGE)\n",
        "# ============================================================================\n",
        "\n",
        "train_csv, val_csv, test_csv = [os.path.join(SPLIT_DIR, f\"{s}.csv\") for s in [\"train\", \"val\", \"test\"]]\n",
        "\n",
        "if all(os.path.exists(f) for f in [train_csv, val_csv, test_csv]):\n",
        "    train_df, val_df, test_df = pd.read_csv(train_csv), pd.read_csv(val_csv), pd.read_csv(test_csv)\n",
        "    print(f\"✓ Loaded splits\")\n",
        "else:\n",
        "    df['book_id'] = df['author'] + '___' + df['file_name']\n",
        "    books = df[['author', 'book_id']].drop_duplicates()\n",
        "\n",
        "    train_books, temp = train_test_split(books, test_size=0.3, stratify=books['author'], random_state=RND)\n",
        "    val_books, test_books = train_test_split(temp, test_size=0.5, stratify=temp['author'], random_state=RND)\n",
        "\n",
        "    train_df = df[df['book_id'].isin(train_books['book_id'])].reset_index(drop=True)\n",
        "    val_df = df[df['book_id'].isin(val_books['book_id'])].reset_index(drop=True)\n",
        "    test_df = df[df['book_id'].isin(test_books['book_id'])].reset_index(drop=True)\n",
        "\n",
        "    # Verify no leakage\n",
        "    assert len(set(train_df['book_id']) & set(test_df['book_id'])) == 0\n",
        "\n",
        "    train_df.to_csv(train_csv, index=False)\n",
        "    val_df.to_csv(val_csv, index=False)\n",
        "    test_df.to_csv(test_csv, index=False)\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrqE7qJHH6P4",
        "outputId": "bda5da68-a9c2-42da-e141-5a1d7f6673df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded splits\n",
            "Train: 10809, Val: 2454, Test: 1935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STYLE FEATURES (300-D, WITH CACHING)\n",
        "# ============================================================================\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "FUNCTION_WORDS = [\"the\",\"be\",\"to\",\"of\",\"and\",\"a\",\"in\",\"that\",\"have\",\"i\",\"it\",\"for\",\"not\",\"on\",\"with\",\"he\",\"as\",\"you\",\"do\",\"at\",\"this\",\"but\",\"his\",\"by\",\"from\",\"they\",\"we\",\"say\",\"her\",\"she\",\"or\",\"an\",\"will\",\"my\",\"one\",\"all\",\"would\",\"there\",\"their\",\"what\",\"so\",\"up\",\"out\",\"if\",\"about\",\"who\",\"get\",\"which\",\"go\",\"me\",\"when\",\"make\",\"can\",\"like\",\"time\",\"no\",\"just\",\"him\",\"know\",\"take\"]\n",
        "\n",
        "def build_style_vector(text, dim=300):\n",
        "    feats = []\n",
        "    words = text.split()\n",
        "    nw = len(words)\n",
        "\n",
        "    # Basic (5)\n",
        "    feats += [len(text), nw, max(1, len(re.split(r'[.!?]+', text))), nw/max(1,len(re.split(r'[.!?]+',text))), sum(len(w) for w in words)/max(1,nw)]\n",
        "\n",
        "    # Punctuation (11)\n",
        "    punc_counts = Counter(ch for ch in text if ch in string.punctuation)\n",
        "    for k in ['.',',',';','!','?',\"'\",'\"',':','-','(',')']: feats.append(punc_counts.get(k,0)/max(1,nw))\n",
        "\n",
        "    # Function words (60)\n",
        "    word_lower = [w.lower() for w in re.findall(r\"\\w+\", text)]\n",
        "    wc = Counter(word_lower)\n",
        "    feats += [wc.get(fw,0)/max(1,len(word_lower)) for fw in FUNCTION_WORDS[:60]]\n",
        "\n",
        "    # Lexical (3)\n",
        "    types = set(word_lower)\n",
        "    feats += [len(types)/max(1,len(word_lower)), sum(1 for _,c in wc.items() if c==1)/max(1,nw), 0.0]\n",
        "\n",
        "    # Syllables (2)\n",
        "    try: sylls = textstat.syllable_count(text)\n",
        "    except: sylls = sum(max(1,sum(1 for c in w if c.lower() in'aeiou')) for w in words)\n",
        "    feats += [sylls, sylls/max(1,nw)]\n",
        "\n",
        "    # Readability (4)\n",
        "    try: feats += [textstat.flesch_reading_ease(text), textstat.flesch_kincaid_grade(text), textstat.smog_index(text), textstat.coleman_liau_index(text)]\n",
        "    except: feats += [0.0]*4\n",
        "\n",
        "    # POS+NER (28)\n",
        "    doc = nlp(text)\n",
        "    pos_c = Counter(t.pos_ for t in doc)\n",
        "    ner_c = Counter(e.label_ for e in doc.ents)\n",
        "    for k in ['NOUN','VERB','ADJ','ADV','PRON','DET','ADP','NUM','PROPN','PART','INTJ','SYM','X','PUNCT']:\n",
        "        feats.append(pos_c.get(k,0)/max(1,len(doc)))\n",
        "    for k in ['PERSON','NORP','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY']:\n",
        "        feats.append(ner_c.get(k,0)/max(1,len(doc)))\n",
        "\n",
        "    # Char n-grams (50)\n",
        "    s = re.sub(r'\\s+','',text.lower())\n",
        "    char_ng = Counter()\n",
        "    for n in [2,3,4]:\n",
        "        for i in range(len(s)-n+1): char_ng[s[i:i+n]]+=1\n",
        "    top = [g for g,_ in char_ng.most_common(50)]\n",
        "    tot = max(1,sum(char_ng.values()))\n",
        "    feats += [char_ng.get(g,0)/tot for g in top]\n",
        "    if len(feats) < 85+50: feats += [0.0]*(85+50-len(feats))\n",
        "\n",
        "    # Word bigrams (50)\n",
        "    word_bg = Counter()\n",
        "    for i in range(len(word_lower)-1): word_bg[\" \".join(word_lower[i:i+2])]+=1\n",
        "    top = [g for g,_ in word_bg.most_common(50)]\n",
        "    tot = max(1,sum(word_bg.values()))\n",
        "    feats += [word_bg.get(g,0)/tot for g in top]\n",
        "    if len(feats) < 185: feats += [0.0]*(185-len(feats))\n",
        "\n",
        "    # Distributional (2)\n",
        "    probs = np.array([v/max(1,sum(wc.values())) for v in wc.values()], dtype=float)\n",
        "    feats += [-np.sum(probs*np.log2(probs+1e-12)), max(wc.values())/max(1,sum(wc.values())) if wc else 0.0]\n",
        "\n",
        "    # Extra (3)\n",
        "    feats += [sum(1 for c in text if c.isdigit())/max(1,len(text)), sum(1 for w in words if w.isupper())/max(1,nw), sum(1 for w in word_lower if w in spacy.lang.en.stop_words.STOP_WORDS)/max(1,len(word_lower))]\n",
        "\n",
        "    feats = feats[:dim] + [0.0]*(dim-len(feats))\n",
        "    return np.array(feats, dtype=np.float32)\n",
        "\n",
        "def cache_features(df, path):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ Loading {os.path.basename(path)}\")\n",
        "        return joblib.load(path)\n",
        "    feats = [build_style_vector(t) for t in tqdm(df['text'], desc=\"Features\")]\n",
        "    joblib.dump(np.stack(feats), path)\n",
        "    return np.stack(feats)\n",
        "\n",
        "train_feats = cache_features(train_df, os.path.join(CACHE_DIR, \"train_feats_rebuild.joblib\"))\n",
        "val_feats = cache_features(val_df, os.path.join(CACHE_DIR, \"val_feats_rebuild.joblib\"))\n",
        "test_feats = cache_features(test_df, os.path.join(CACHE_DIR, \"test_feats_rebuild.joblib\"))\n",
        "\n",
        "# Embeddings - UPDATED MODEL\n",
        "emb_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
        "\n",
        "def cache_embeddings(df, path):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ Loading {os.path.basename(path)}\")\n",
        "        return joblib.load(path)\n",
        "    embs = emb_model.encode(df['text'].tolist(), show_progress_bar=True, batch_size=64, convert_to_numpy=True)\n",
        "    joblib.dump(embs, path)\n",
        "    return embs\n",
        "\n",
        "train_embs = cache_embeddings(train_df, os.path.join(CACHE_DIR, \"train_embs_rebuild.joblib\"))\n",
        "val_embs = cache_embeddings(val_df, os.path.join(CACHE_DIR, \"val_embs_rebuild.joblib\"))\n",
        "test_embs = cache_embeddings(test_df, os.path.join(CACHE_DIR, \"test_embs_rebuild.joblib\"))\n",
        "\n",
        "print(f\"Features: {train_feats.shape}, Embeddings: {train_embs.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulwnvVp3IO_8",
        "outputId": "acd886e2-f943-40f4-a6db-92a53c9dcd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loading train_feats_rebuild.joblib\n",
            "✓ Loading val_feats_rebuild.joblib\n",
            "✓ Loading test_feats_rebuild.joblib\n",
            "✓ Loading train_embs_rebuild.joblib\n",
            "✓ Loading val_embs_rebuild.joblib\n",
            "✓ Loading test_embs_rebuild.joblib\n",
            "Features: (10809, 300), Embeddings: (10809, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VADES MODEL (SIMPLIFIED, WORKING VERSION)\n",
        "# ============================================================================\n",
        "\n",
        "class VADES(nn.Module):\n",
        "    def __init__(self, emb_dim=768, style_dim=300, hidden=512, n_authors=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Document encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(hidden, style_dim)\n",
        "        )\n",
        "\n",
        "        # Author embeddings (simple, no variance)\n",
        "        self.authors = nn.Embedding(n_authors, style_dim)\n",
        "        nn.init.xavier_uniform_(self.authors.weight)\n",
        "\n",
        "    def forward(self, doc_emb):\n",
        "        return self.encoder(doc_emb)\n",
        "\n",
        "# Simple contrastive loss (no VIB complexity)\n",
        "def simple_contrastive_loss(model, doc_emb, author_ids, style_feats, neg_author_ids, neg_style_feats, alpha=0.1, margin=1.0):\n",
        "\n",
        "    # Get representations\n",
        "    doc_emb_out = model(doc_emb)  # [batch, 300]\n",
        "    author_emb = model.authors(author_ids)  # [batch, 300]\n",
        "\n",
        "    # Positive distances (should be small)\n",
        "    pos_dist_author = torch.sum((doc_emb_out - author_emb) ** 2, dim=1)  # [batch]\n",
        "    pos_dist_style = torch.sum((doc_emb_out - style_feats) ** 2, dim=1)  # [batch]\n",
        "\n",
        "    # Negative distances (should be large)\n",
        "    batch_size = doc_emb.size(0)\n",
        "    k_neg = neg_author_ids.size(1)\n",
        "\n",
        "    neg_author_emb = model.authors(neg_author_ids.view(-1)).view(batch_size, k_neg, -1)  # [batch, k, 300]\n",
        "    doc_exp = doc_emb_out.unsqueeze(1)  # [batch, 1, 300]\n",
        "    neg_dist_author = torch.sum((doc_exp - neg_author_emb) ** 2, dim=2)  # [batch, k]\n",
        "\n",
        "    neg_style_exp = neg_style_feats  # [batch, k, 300]\n",
        "    neg_dist_style = torch.sum((doc_exp - neg_style_exp) ** 2, dim=2)  # [batch, k]\n",
        "\n",
        "    # Triplet-like loss: positive should be < negative\n",
        "    author_loss = torch.relu(pos_dist_author.unsqueeze(1) - neg_dist_author + margin).mean()\n",
        "    style_loss = torch.relu(pos_dist_style.unsqueeze(1) - neg_dist_style + margin).mean()\n",
        "\n",
        "    total_loss = (1 - alpha) * author_loss + alpha * style_loss\n",
        "\n",
        "    return total_loss, author_loss, style_loss"
      ],
      "metadata": {
        "id": "3csRjdCKIVjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================================\n",
        "# # VADES MODEL (COSINE-BASED VERSION)\n",
        "# # ============================================================================\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class VADES(nn.Module):\n",
        "#     def __init__(self, emb_dim=768, style_dim=300, hidden=512, n_authors=10):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # Document encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Linear(emb_dim, hidden),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.5),      # Stronger regularization for small datasets\n",
        "#             nn.Linear(hidden, style_dim)\n",
        "#         )\n",
        "\n",
        "#         # Author embeddings\n",
        "#         self.authors = nn.Embedding(n_authors, style_dim)\n",
        "#         nn.init.xavier_uniform_(self.authors.weight)\n",
        "\n",
        "#     def forward(self, doc_emb):\n",
        "#         return self.encoder(doc_emb)\n",
        "\n",
        "\n",
        "# # ============================================================================\n",
        "# # COSINE CONTRASTIVE LOSS\n",
        "# # ============================================================================\n",
        "\n",
        "# def cosine_contrastive_loss(model, doc_emb, author_ids, style_feats,\n",
        "#                             neg_author_ids, neg_style_feats, alpha=0.1, margin=0.5):\n",
        "#     \"\"\"\n",
        "#     Contrastive loss based on cosine similarity.\n",
        "#     Encourages doc embeddings to align with their author & style,\n",
        "#     and diverge from negatives.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Encode documents\n",
        "#     doc_emb_out = model(doc_emb)  # [batch, 300]\n",
        "#     author_emb = model.authors(author_ids)  # [batch, 300]\n",
        "\n",
        "#     batch_size = doc_emb.size(0)\n",
        "#     k_neg = neg_author_ids.size(1)\n",
        "\n",
        "#     # Negative author embeddings\n",
        "#     neg_author_emb = model.authors(neg_author_ids.view(-1)).view(batch_size, k_neg, -1)\n",
        "\n",
        "#     # Normalize all embeddings (critical for cosine similarity)\n",
        "#     doc_emb_out = F.normalize(doc_emb_out, p=2, dim=1)\n",
        "#     author_emb = F.normalize(author_emb, p=2, dim=1)\n",
        "#     style_feats = F.normalize(style_feats, p=2, dim=1)\n",
        "#     neg_author_emb = F.normalize(neg_author_emb, p=2, dim=2)\n",
        "#     neg_style_feats = F.normalize(neg_style_feats, p=2, dim=2)\n",
        "\n",
        "#     # ---- Positive cosine distances (want them small) ----\n",
        "#     pos_dist_author = 1 - F.cosine_similarity(doc_emb_out, author_emb)  # [batch]\n",
        "#     pos_dist_style = 1 - F.cosine_similarity(doc_emb_out, style_feats)  # [batch]\n",
        "\n",
        "#     # ---- Negative cosine distances (want them large) ----\n",
        "#     doc_exp = doc_emb_out.unsqueeze(1)  # [batch, 1, 300]\n",
        "#     # Cosine similarity via batched matrix multiplication\n",
        "#     neg_sim_author = torch.bmm(neg_author_emb, doc_exp.transpose(1, 2)).squeeze(2)  # [batch, k]\n",
        "#     neg_sim_style = torch.bmm(neg_style_feats, doc_exp.transpose(1, 2)).squeeze(2)  # [batch, k]\n",
        "\n",
        "#     neg_dist_author = 1 - neg_sim_author\n",
        "#     neg_dist_style = 1 - neg_sim_style\n",
        "\n",
        "#     # ---- Triplet-like margin loss ----\n",
        "#     author_loss = torch.relu(pos_dist_author.unsqueeze(1) - neg_dist_author + margin).mean()\n",
        "#     style_loss = torch.relu(pos_dist_style.unsqueeze(1) - neg_dist_style + margin).mean()\n",
        "\n",
        "#     total_loss = (1 - alpha) * author_loss + alpha * style_loss\n",
        "\n",
        "#     return total_loss, author_loss, style_loss\n"
      ],
      "metadata": {
        "id": "ZPpmjjDZW2La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, embs, feats, aids):\n",
        "        self.embs = torch.FloatTensor(embs)\n",
        "        self.feats = torch.FloatTensor(feats)\n",
        "        self.aids = torch.LongTensor(aids)\n",
        "    def __len__(self): return len(self.embs)\n",
        "    def __getitem__(self, i): return self.embs[i], self.feats[i], self.aids[i]\n",
        "\n",
        "author2id = {a: i for i, a in enumerate(sorted(train_df['author'].unique()))}\n",
        "num_authors = len(author2id)\n",
        "\n",
        "train_aids = np.array([author2id[a] for a in train_df['author']])\n",
        "val_aids = np.array([author2id.get(a,-1) for a in val_df['author']])\n",
        "test_aids = np.array([author2id.get(a,-1) for a in test_df['author']])\n",
        "\n",
        "val_mask, test_mask = val_aids>=0, test_aids>=0\n",
        "val_embs, val_feats, val_aids = val_embs[val_mask], val_feats[val_mask], val_aids[val_mask]\n",
        "test_embs, test_feats, test_aids = test_embs[test_mask], test_feats[test_mask], test_aids[test_mask]\n",
        "\n",
        "BATCH = 32\n",
        "train_loader = DataLoader(SimpleDataset(train_embs, train_feats, train_aids), BATCH, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(SimpleDataset(val_embs, val_feats, val_aids), BATCH, shuffle=False)\n",
        "test_loader = DataLoader(SimpleDataset(test_embs, test_feats, test_aids), BATCH, shuffle=False)\n",
        "\n",
        "print(f\"Authors: {num_authors}, Train batches: {len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3KFOW7iIcfm",
        "outputId": "773cecfe-1a09-4aad-97ec-a7755fb8e6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authors: 10, Train batches: 337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "model = VADES(n_authors=num_authors).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-7)\n",
        "\n",
        "EPOCHS, ALPHA, MARGIN, K_NEG = 15, 0.1, 0.5, 5\n",
        "\n",
        "print(f\"\\n{'='*80}\\nTRAINING (Simplified)\\n{'='*80}\")\n",
        "print(f\"Epochs: {EPOCHS}, Alpha: {ALPHA}, Margin: {MARGIN}, K_neg: {K_NEG}\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for doc_emb, style_feat, aid in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        doc_emb, style_feat, aid = doc_emb.to(device), style_feat.to(device), aid.to(device)\n",
        "\n",
        "        bs = doc_emb.size(0)\n",
        "\n",
        "        # Sample negatives\n",
        "        neg_aids = torch.randint(0, num_authors, (bs, K_NEG), device=device)\n",
        "        for i in range(bs):\n",
        "            for k in range(K_NEG):\n",
        "                while neg_aids[i,k] == aid[i]:\n",
        "                    neg_aids[i,k] = torch.randint(0, num_authors, (1,), device=device)\n",
        "\n",
        "        idx = torch.randperm(bs, device=device)\n",
        "        neg_style = torch.stack([style_feat[torch.roll(idx,k+1)] for k in range(K_NEG)], dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, a_loss, s_loss = simple_contrastive_loss(model, doc_emb, aid, style_feat, neg_aids, neg_style, ALPHA, MARGIN)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if epoch == 0 and len(losses) == 1:\n",
        "            print(f\"First batch - Loss: {loss.item():.4f}, Author: {a_loss.item():.4f}, Style: {s_loss.item():.4f}\")\n",
        "\n",
        "    # Val\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for doc_emb, style_feat, aid in val_loader:\n",
        "            doc_emb, style_feat, aid = doc_emb.to(device), style_feat.to(device), aid.to(device)\n",
        "            doc_out = model(doc_emb)\n",
        "            author_emb = model.authors(aid)\n",
        "            val_loss = torch.sum((doc_out - author_emb)**2, dim=1).mean()\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    avg_train, avg_val = np.mean(losses), np.mean(val_losses)\n",
        "    print(f\"Epoch {epoch+1}: Train={avg_train:.4f}, Val={avg_val:.4f}\")\n",
        "\n",
        "    if avg_val < best_val_loss:\n",
        "        best_val_loss = avg_val\n",
        "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, 'best.pth'))\n",
        "        print(f\"  ✅ Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc0lJU50CeNc",
        "outputId": "85ea52cb-69a2-4a3b-c7ff-ad6ad3c19c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING (Simplified)\n",
            "================================================================================\n",
            "Epochs: 15, Alpha: 0.1, Margin: 0.5, K_neg: 5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   3%|▎         | 10/337 [00:00<00:16, 19.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch - Loss: 1916.1473, Author: 0.4774, Style: 19157.1758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 337/337 [00:04<00:00, 73.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train=4666.5006, Val=2.2503\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 337/337 [00:03<00:00, 106.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train=4754.1899, Val=2.2494\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 337/337 [00:03<00:00, 106.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train=4704.1279, Val=2.2486\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 337/337 [00:03<00:00, 85.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train=4783.5373, Val=2.2478\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 337/337 [00:03<00:00, 93.86it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train=4716.6498, Val=2.2470\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 337/337 [00:03<00:00, 107.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train=4735.9640, Val=2.2463\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 337/337 [00:03<00:00, 87.13it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train=4722.0411, Val=2.2455\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 337/337 [00:03<00:00, 85.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train=4677.5039, Val=2.2448\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 337/337 [00:03<00:00, 105.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train=4665.0687, Val=2.2441\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 337/337 [00:03<00:00, 106.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train=4736.3060, Val=2.2434\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 337/337 [00:03<00:00, 106.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Train=4757.5253, Val=2.2427\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 337/337 [00:03<00:00, 86.46it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Train=4740.0651, Val=2.2420\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 337/337 [00:03<00:00, 105.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Train=4703.7472, Val=2.2414\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 337/337 [00:03<00:00, 106.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Train=4734.2384, Val=2.2408\n",
            "  ✅ Saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 337/337 [00:03<00:00, 99.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Train=4669.3135, Val=2.2402\n",
            "  ✅ Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join(CKPT_DIR, 'best.pth')))\n",
        "model.eval()\n",
        "\n",
        "# Authorship\n",
        "author_means = defaultdict(list)\n",
        "with torch.no_grad():\n",
        "    for doc_emb, _, aid in train_loader:\n",
        "        doc_emb = doc_emb.to(device)\n",
        "        doc_out = model(doc_emb)\n",
        "        for i, a in enumerate(aid.numpy()):\n",
        "            author_means[a].append(doc_out[i].cpu().numpy())\n",
        "\n",
        "author_means = {a: np.mean(embs, axis=0) for a, embs in author_means.items()}\n",
        "\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for doc_emb, _, aid in test_loader:\n",
        "        doc_emb = doc_emb.to(device)\n",
        "        doc_out = model(doc_emb).cpu().numpy()\n",
        "        for i, true_a in enumerate(aid.numpy()):\n",
        "            dists = {a: np.linalg.norm(doc_out[i] - m) for a, m in author_means.items()}\n",
        "            if min(dists, key=dists.get) == true_a: correct += 1\n",
        "            total += 1\n",
        "\n",
        "print(f\"\\n{'='*80}\\nRESULTS\\n{'='*80}\")\n",
        "print(f\"Accuracy: {100*correct/total:.2f}% ({correct}/{total})\")\n",
        "print(f\"Random: {100/num_authors:.2f}%\")\n",
        "print(f\"\\n✅ DONE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Briv50u5Ikur",
        "outputId": "177f69d6-7cd7-49e9-aab4-001ec1dda8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RESULTS\n",
            "================================================================================\n",
            "Accuracy: 39.22% (759/1935)\n",
            "Random: 10.00%\n",
            "\n",
            "✅ DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdMYsYNJKtRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}